{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Vectorization with Azure AI Search Integrated Vectorization\n",
    "\n",
    "This notebook handles:\n",
    "1. **Document Retrieval** - Download processed documents from Azure Blob Storage\n",
    "2. **Search Index Creation** - Create Azure AI Search index with integrated vectorization\n",
    "3. **Document Upload** - Upload documents directly to Azure AI Search\n",
    "4. **Integrated Vectorization** - Let Azure AI Search handle embedding generation automatically\n",
    "5. **Testing** - Test the search index with semantic queries\n",
    "\n",
    "## Prerequisites\n",
    "- Completed Challenge 1: Document Processing\n",
    "- Azure AI Search service deployed (with integrated vectorization support)\n",
    "- Azure OpenAI service with text-embedding-ada-002 model deployed\n",
    "- Processed documents available in 'processed-documents' container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All imports successful!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "\n",
    "# Azure SDK imports\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    VectorSearch,\n",
    "    VectorSearchProfile,\n",
    "    VectorSearchAlgorithmConfiguration,\n",
    "    VectorSearchAlgorithmKind,\n",
    "    SemanticConfiguration,\n",
    "    SemanticPrioritizedFields,\n",
    "    SemanticField,\n",
    "    SemanticSearch,\n",
    "    AzureOpenAIVectorizer,\n",
    "    AzureOpenAIVectorizerParameters,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    ExhaustiveKnnAlgorithmConfiguration\n",
    ")\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.core.exceptions import ResourceNotFoundError\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(\"✅ All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configuration loaded successfully!\n",
      "🔍 Search Service: msagthack-search-kl2yy7velhg4u\n",
      "🔗 Search Endpoint: https://msagthack-search-kl2yy7velhg4u.search.windows.net/\n",
      "📦 Processed Documents Container: processed-documents\n",
      "📇 Search Index: insurance-documents-index\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "class Config:\n",
    "    # Storage configuration\n",
    "    AZURE_STORAGE_CONNECTION_STRING = os.getenv('AZURE_STORAGE_CONNECTION_STRING')\n",
    "    \n",
    "    # Azure AI Search configuration\n",
    "    SEARCH_SERVICE_NAME = os.getenv('SEARCH_SERVICE_NAME')\n",
    "    SEARCH_SERVICE_ENDPOINT = os.getenv('SEARCH_SERVICE_ENDPOINT')\n",
    "    SEARCH_ADMIN_KEY = os.getenv('SEARCH_ADMIN_KEY')\n",
    "    \n",
    "    # Azure OpenAI configuration (for integrated vectorization)\n",
    "    AZURE_OPENAI_ENDPOINT = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "    AZURE_OPENAI_API_KEY = os.getenv('AZURE_OPENAI_KEY')\n",
    "    AZURE_OPENAI_EMBEDDING_DEPLOYMENT = os.getenv('AZURE_OPENAI_EMBEDDING_DEPLOYMENT', 'text-embedding-ada-002')\n",
    "    \n",
    "    # Container names\n",
    "    PROCESSED_CONTAINER = 'processed-documents'\n",
    "    \n",
    "    # Search index configuration\n",
    "    SEARCH_INDEX_NAME = 'insurance-documents-index'\n",
    "    CHUNK_SIZE = 1000  # Characters per chunk\n",
    "    CHUNK_OVERLAP = 200  # Overlap between chunks\n",
    "\n",
    "# Validate configuration\n",
    "required_vars = [\n",
    "    Config.AZURE_STORAGE_CONNECTION_STRING,\n",
    "    Config.SEARCH_SERVICE_ENDPOINT,\n",
    "    Config.SEARCH_ADMIN_KEY,\n",
    "    Config.AZURE_OPENAI_ENDPOINT,\n",
    "    Config.AZURE_OPENAI_API_KEY\n",
    "]\n",
    "\n",
    "missing_vars = [var for var in required_vars if not var]\n",
    "if missing_vars:\n",
    "    print(\"❌ Missing environment variables. Please check your .env file.\")\n",
    "    print(\"Missing variables:\")\n",
    "    if not Config.SEARCH_SERVICE_ENDPOINT:\n",
    "        print(\"  - SEARCH_SERVICE_ENDPOINT\")\n",
    "    if not Config.SEARCH_ADMIN_KEY:\n",
    "        print(\"  - SEARCH_ADMIN_KEY\")\n",
    "    if not Config.AZURE_OPENAI_ENDPOINT:\n",
    "        print(\"  - AZURE_OPENAI_ENDPOINT\")\n",
    "    if not Config.AZURE_OPENAI_API_KEY:\n",
    "        print(\"  - AZURE_OPENAI_API_KEY\")\n",
    "    if not Config.AZURE_STORAGE_CONNECTION_STRING:\n",
    "        print(\"  - AZURE_STORAGE_CONNECTION_STRING\")\n",
    "else:\n",
    "    print(\"✅ Configuration loaded successfully!\")\n",
    "    print(f\"🔍 Search Service: {Config.SEARCH_SERVICE_NAME}\")\n",
    "    print(f\"🔗 Search Endpoint: {Config.SEARCH_SERVICE_ENDPOINT}\")\n",
    "    print(f\"📦 Processed Documents Container: {Config.PROCESSED_CONTAINER}\")\n",
    "    print(f\"📇 Search Index: {Config.SEARCH_INDEX_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Azure Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to Blob Storage - Found 4 containers\n",
      "✅ Connected to Azure AI Search - Storage used: Unknown\n"
     ]
    }
   ],
   "source": [
    "# Initialize Azure clients\n",
    "def initialize_clients():\n",
    "    \"\"\"Initialize Azure service clients\"\"\"\n",
    "    try:\n",
    "        # Blob Storage client\n",
    "        blob_service_client = BlobServiceClient.from_connection_string(\n",
    "            Config.AZURE_STORAGE_CONNECTION_STRING\n",
    "        )\n",
    "        \n",
    "        # Azure AI Search clients\n",
    "        search_credential = AzureKeyCredential(Config.SEARCH_ADMIN_KEY)\n",
    "        \n",
    "        search_index_client = SearchIndexClient(\n",
    "            endpoint=Config.SEARCH_SERVICE_ENDPOINT,\n",
    "            credential=search_credential\n",
    "        )\n",
    "        \n",
    "        search_client = SearchClient(\n",
    "            endpoint=Config.SEARCH_SERVICE_ENDPOINT,\n",
    "            index_name=Config.SEARCH_INDEX_NAME,\n",
    "            credential=search_credential\n",
    "        )\n",
    "        \n",
    "        # Test the connections\n",
    "        containers = list(blob_service_client.list_containers())\n",
    "        print(f\"✅ Connected to Blob Storage - Found {len(containers)} containers\")\n",
    "        \n",
    "        # Test search service (fixed the storage_size access)\n",
    "        try:\n",
    "            service_stats = search_index_client.get_service_statistics()\n",
    "            storage_used = getattr(service_stats, 'storage_size', 'Unknown')\n",
    "            print(f\"✅ Connected to Azure AI Search - Storage used: {storage_used}\")\n",
    "        except Exception as e:\n",
    "            print(f\"✅ Connected to Azure AI Search - Service is available\")\n",
    "            print(f\"   (Note: Could not get statistics: {e})\")\n",
    "        \n",
    "        return blob_service_client, search_index_client, search_client\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error initializing clients: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# Initialize clients\n",
    "blob_service_client, search_index_client, search_client = initialize_clients()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Azure AI Search Index with Integrated Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 Original endpoint: https://msagthack-aifoundry-kl2yy7velhg4u.cognitiveservices.azure.com/\n",
      "🔗 Formatted endpoint: https://msagthack-aifoundry-kl2yy7velhg4u.openai.azure.com\n",
      "🚀 Using deployment: text-embedding-ada-002\n",
      "✅ Search index 'insurance-documents-index' created successfully!\n",
      "📋 Index fields: 12\n",
      "🔍 Vector search enabled: True\n",
      "🧠 Semantic search enabled: True\n",
      "\n",
      "📊 Index created successfully!\n"
     ]
    }
   ],
   "source": [
    "class SearchIndexManager:\n",
    "    \"\"\"Class to manage Azure AI Search index with integrated vectorization\"\"\"\n",
    "    \n",
    "    def __init__(self, search_index_client: SearchIndexClient):\n",
    "        self.search_index_client = search_index_client\n",
    "        self.index_name = Config.SEARCH_INDEX_NAME\n",
    "\n",
    "    def _format_azure_openai_endpoint(self, endpoint: str) -> str:\n",
    "        \"\"\"Format the Azure OpenAI endpoint for use with Azure AI Search vectorizer\"\"\"\n",
    "        # Remove trailing slash if present\n",
    "        endpoint = endpoint.rstrip('/')\n",
    "        \n",
    "        # Check if it already has the correct format\n",
    "        if endpoint.endswith('.openai.azure.com'):\n",
    "            return endpoint\n",
    "        \n",
    "        # Extract the resource name from various possible formats\n",
    "        if '.cognitiveservices.azure.com' in endpoint:\n",
    "            # Convert from cognitive services format to OpenAI format\n",
    "            resource_name = endpoint.split('.')[0].split('//')[-1]\n",
    "            return f\"https://{resource_name}.openai.azure.com\"\n",
    "        elif '/openai/' in endpoint:\n",
    "            # Extract resource name from URL with /openai/ path\n",
    "            parts = endpoint.split('/')\n",
    "            resource_name = parts[2].split('.')[0]\n",
    "            return f\"https://{resource_name}.openai.azure.com\"\n",
    "        else:\n",
    "            # Try to extract resource name and format correctly\n",
    "            if 'https://' in endpoint:\n",
    "                resource_name = endpoint.split('//')[1].split('.')[0]\n",
    "            else:\n",
    "                resource_name = endpoint.split('.')[0]\n",
    "            return f\"https://{resource_name}.openai.azure.com\"\n",
    "    \n",
    "    def create_search_index(self) -> bool:\n",
    "        \"\"\"Create a search index with integrated vectorization\"\"\"\n",
    "        try:\n",
    "            # Format the Azure OpenAI endpoint correctly\n",
    "            formatted_endpoint = self._format_azure_openai_endpoint(Config.AZURE_OPENAI_ENDPOINT)\n",
    "            print(f\"🔗 Original endpoint: {Config.AZURE_OPENAI_ENDPOINT}\")\n",
    "            print(f\"🔗 Formatted endpoint: {formatted_endpoint}\")\n",
    "            print(f\"🚀 Using deployment: {Config.AZURE_OPENAI_EMBEDDING_DEPLOYMENT}\")\n",
    "            \n",
    "            # Define the vectorizer for integrated vectorization\n",
    "            vectorizer = AzureOpenAIVectorizer(\n",
    "                vectorizer_name=\"insurance-vectorizer\",\n",
    "                parameters=AzureOpenAIVectorizerParameters(\n",
    "                    resource_url=formatted_endpoint,  # Use formatted endpoint\n",
    "                    deployment_name=Config.AZURE_OPENAI_EMBEDDING_DEPLOYMENT,  # Use config variable\n",
    "                    model_name=\"text-embedding-ada-002\",\n",
    "                    api_key=Config.AZURE_OPENAI_API_KEY\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Define vector search configuration\n",
    "            vector_search = VectorSearch(\n",
    "                algorithms=[\n",
    "                    HnswAlgorithmConfiguration(name=\"insurance-algorithm\", kind=\"hnsw\"),\n",
    "                    ExhaustiveKnnAlgorithmConfiguration(name=\"my-eknn-vector-config\", kind=\"exhaustiveKnn\")\n",
    "                ],\n",
    "                profiles=[\n",
    "                    VectorSearchProfile(\n",
    "                        name=\"insurance-profile\",\n",
    "                        algorithm_configuration_name=\"insurance-algorithm\",\n",
    "                        vectorizer_name=\"insurance-vectorizer\"\n",
    "                    )\n",
    "                ],\n",
    "                vectorizers=[vectorizer]\n",
    "            )\n",
    "            \n",
    "            # Define semantic search configuration\n",
    "            semantic_config = SemanticConfiguration(\n",
    "                name=\"insurance-semantic\",  # Fixed to match SearchTester\n",
    "                prioritized_fields=SemanticPrioritizedFields(\n",
    "                    title_field=SemanticField(field_name=\"title\"),\n",
    "                    content_fields=[SemanticField(field_name=\"content\")],\n",
    "                    keywords_fields=[\n",
    "                        SemanticField(field_name=\"category\"),\n",
    "                        SemanticField(field_name=\"file_name\")\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            semantic_search = SemanticSearch(\n",
    "                configurations=[semantic_config]\n",
    "            )\n",
    "            \n",
    "            # Define the search index schema\n",
    "            fields = [\n",
    "                SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
    "                SearchableField(name=\"title\", type=SearchFieldDataType.String),\n",
    "                SearchableField(name=\"content\", type=SearchFieldDataType.String),\n",
    "                SearchableField(name=\"category\", type=SearchFieldDataType.String, filterable=True, facetable=True),\n",
    "                SearchableField(name=\"file_name\", type=SearchFieldDataType.String, filterable=True),\n",
    "                SimpleField(name=\"file_type\", type=SearchFieldDataType.String, filterable=True),\n",
    "                SimpleField(name=\"chunk_id\", type=SearchFieldDataType.Int32),\n",
    "                SimpleField(name=\"chunk_count\", type=SearchFieldDataType.Int32),\n",
    "                SimpleField(name=\"original_length\", type=SearchFieldDataType.Int32),\n",
    "                SimpleField(name=\"chunk_length\", type=SearchFieldDataType.Int32),\n",
    "                SimpleField(name=\"processing_date\", type=SearchFieldDataType.DateTimeOffset),\n",
    "                \n",
    "                # Vector field for integrated vectorization\n",
    "                SearchField(\n",
    "                    name=\"content_vector\",\n",
    "                    type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                    searchable=True,\n",
    "                    vector_search_dimensions=1536,  # ada-002 embedding dimension\n",
    "                    vector_search_profile_name=\"insurance-profile\"\n",
    "                )\n",
    "            ]\n",
    "            \n",
    "            # Create the search index\n",
    "            index = SearchIndex(\n",
    "                name=self.index_name,\n",
    "                fields=fields,\n",
    "                vector_search=vector_search,\n",
    "                semantic_search=semantic_search\n",
    "            )\n",
    "            \n",
    "            # Create or update the index\n",
    "            result = self.search_index_client.create_or_update_index(index)\n",
    "            print(f\"✅ Search index '{self.index_name}' created successfully!\")\n",
    "            print(f\"📋 Index fields: {len(result.fields)}\")\n",
    "            print(f\"🔍 Vector search enabled: {bool(result.vector_search)}\")\n",
    "            print(f\"🧠 Semantic search enabled: {bool(result.semantic_search)}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error creating search index: {e}\")\n",
    "            print(f\"🔍 Debug info:\")\n",
    "            print(f\"   - Original endpoint: {Config.AZURE_OPENAI_ENDPOINT}\")\n",
    "            print(f\"   - Deployment name: {Config.AZURE_OPENAI_EMBEDDING_DEPLOYMENT}\")\n",
    "            print(f\"   - API key present: {bool(Config.AZURE_OPENAI_API_KEY)}\")\n",
    "            \n",
    "            # Add more detailed error information\n",
    "            import traceback\n",
    "            print(f\"📋 Full error details:\\n{traceback.format_exc()}\")\n",
    "            return False\n",
    "    \n",
    "    def delete_index_if_exists(self) -> bool:\n",
    "        \"\"\"Delete the index if it exists\"\"\"\n",
    "        try:\n",
    "            self.search_index_client.delete_index(self.index_name)\n",
    "            print(f\"✅ Deleted existing index: {self.index_name}\")\n",
    "            return True\n",
    "        except ResourceNotFoundError:\n",
    "            print(f\"ℹ️ Index {self.index_name} doesn't exist - will create new\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error deleting index: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def get_index_stats(self) -> Dict:\n",
    "        \"\"\"Get statistics about the search index\"\"\"\n",
    "        try:\n",
    "            index = self.search_index_client.get_index(self.index_name)\n",
    "            stats = self.search_index_client.get_index_statistics(self.index_name)\n",
    "            \n",
    "            # Handle both object and dictionary responses\n",
    "            if hasattr(stats, 'document_count'):\n",
    "                # Object response\n",
    "                return {\n",
    "                    \"name\": index.name,\n",
    "                    \"field_count\": len(index.fields),\n",
    "                    \"document_count\": stats.document_count,\n",
    "                    \"storage_size\": stats.storage_size,\n",
    "                    \"vector_index_size\": getattr(stats, 'vector_index_size', 0)\n",
    "                }\n",
    "            else:\n",
    "                # Dictionary response\n",
    "                return {\n",
    "                    \"name\": index.name,\n",
    "                    \"field_count\": len(index.fields),\n",
    "                    \"document_count\": stats.get('document_count', 0),\n",
    "                    \"storage_size\": stats.get('storage_size', 0),\n",
    "                    \"vector_index_size\": stats.get('vector_index_size', 0)\n",
    "                }\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error getting index stats: {e}\")\n",
    "            return {}\n",
    "\n",
    "# Initialize search index manager\n",
    "if search_index_client:\n",
    "    index_manager = SearchIndexManager(search_index_client)\n",
    "    \n",
    "    # Option to recreate index (uncomment if needed)\n",
    "    # print(\"🔄 Recreating search index...\")\n",
    "    # index_manager.delete_index_if_exists()\n",
    "    \n",
    "    success = index_manager.create_search_index()\n",
    "    if success:\n",
    "        print(\"\\n📊 Index created successfully!\")\n",
    "    else:\n",
    "        print(\"\\n❌ Failed to create search index\")\n",
    "else:\n",
    "    print(\"❌ Cannot create search index - missing search client\")\n",
    "    index_manager = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Document Retrieval and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Document processors initialized\n"
     ]
    }
   ],
   "source": [
    "# Reuse the DocumentRetriever class from previous notebook\n",
    "class DocumentRetriever:\n",
    "    \"\"\"Class to handle document retrieval from blob storage\"\"\"\n",
    "    \n",
    "    def __init__(self, blob_service_client):\n",
    "        self.blob_service_client = blob_service_client\n",
    "    \n",
    "    def get_all_processed_documents(self) -> Dict:\n",
    "        \"\"\"Get all processed documents ready for vectorization\"\"\"\n",
    "        try:\n",
    "            container_client = self.blob_service_client.get_container_client(Config.PROCESSED_CONTAINER)\n",
    "            blob_client = container_client.get_blob_client(\"processed_documents_for_vectorization.json\")\n",
    "            \n",
    "            blob_data = blob_client.download_blob().readall()\n",
    "            documents = json.loads(blob_data.decode('utf-8'))\n",
    "            \n",
    "            print(f\"✅ Downloaded processed documents\")\n",
    "            return documents\n",
    "                \n",
    "        except ResourceNotFoundError:\n",
    "            print(f\"❌ File not found: processed_documents_for_vectorization.json\")\n",
    "            return {}\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error downloading documents: {e}\")\n",
    "            return {}\n",
    "\n",
    "# Text chunking class (simplified for search index)\n",
    "class TextChunker:\n",
    "    \"\"\"Class to handle intelligent text chunking for search index\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize text\"\"\"\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'\\n+', '\\n', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    def chunk_text_for_search(self, text: str, metadata: Dict) -> List[Dict]:\n",
    "        \"\"\"Create chunks optimized for search index\"\"\"\n",
    "        text = self.clean_text(text)\n",
    "        chunks = []\n",
    "        \n",
    "        if len(text) <= self.chunk_size:\n",
    "            return [{\n",
    "                'content': text,\n",
    "                'chunk_id': 0,\n",
    "                'chunk_count': 1,\n",
    "                'metadata': metadata.copy()\n",
    "            }]\n",
    "        \n",
    "        # Simple sliding window chunking\n",
    "        start = 0\n",
    "        chunk_id = 0\n",
    "        \n",
    "        while start < len(text):\n",
    "            end = start + self.chunk_size\n",
    "            \n",
    "            # Try to break at sentence boundaries\n",
    "            if end < len(text):\n",
    "                sentence_end = text.rfind('.', start, end)\n",
    "                if sentence_end > start:\n",
    "                    end = sentence_end + 1\n",
    "            \n",
    "            chunk_text = text[start:end].strip()\n",
    "            \n",
    "            if chunk_text:\n",
    "                chunks.append({\n",
    "                    'content': chunk_text,\n",
    "                    'chunk_id': chunk_id,\n",
    "                    'chunk_count': 0,  # Will be updated later\n",
    "                    'metadata': metadata.copy()\n",
    "                })\n",
    "                chunk_id += 1\n",
    "            \n",
    "            # Move start position with overlap\n",
    "            start = max(start + self.chunk_size - self.chunk_overlap, end)\n",
    "        \n",
    "        # Update chunk count\n",
    "        for chunk in chunks:\n",
    "            chunk['chunk_count'] = len(chunks)\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "# Initialize processors\n",
    "if blob_service_client:\n",
    "    retriever = DocumentRetriever(blob_service_client)\n",
    "    chunker = TextChunker(\n",
    "        chunk_size=Config.CHUNK_SIZE,\n",
    "        chunk_overlap=Config.CHUNK_OVERLAP\n",
    "    )\n",
    "    print(\"✅ Document processors initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Retrieve and Process Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Retrieving processed documents from blob storage...\n",
      "============================================================\n",
      "✅ Downloaded processed documents\n",
      "\n",
      "✅ Successfully retrieved processed documents!\n",
      "📊 Document categories: ['policies', 'claims', 'statements']\n",
      "\n",
      "📂 Processing policies documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing policies: 100%|██████████| 5/5 [00:00<00:00, 626.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 Processing claims documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing claims: 100%|██████████| 6/6 [00:00<00:00, 2084.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 Processing statements documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing statements: 100%|██████████| 5/5 [00:00<00:00, 4247.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Prepared 77 documents for search index\n",
      "   - policies: 44 chunks from 5 files\n",
      "   - claims: 18 chunks from 6 files\n",
      "   - statements: 15 chunks from 5 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrieve processed documents\n",
    "print(\"📥 Retrieving processed documents from blob storage...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "processed_documents = retriever.get_all_processed_documents()\n",
    "\n",
    "if processed_documents:\n",
    "    print(f\"\\n✅ Successfully retrieved processed documents!\")\n",
    "    print(f\"📊 Document categories: {list(processed_documents.keys())}\")\n",
    "    \n",
    "    # Process documents into search-ready chunks\n",
    "    search_documents = []\n",
    "    \n",
    "    for category, docs in processed_documents.items():\n",
    "        print(f\"\\n📂 Processing {category} documents...\")\n",
    "        \n",
    "        for doc in tqdm(docs, desc=f\"Processing {category}\"):\n",
    "            if not doc.get('success', False):\n",
    "                continue\n",
    "            \n",
    "            # Get text content\n",
    "            text_content = doc.get('text') or doc.get('description', '')\n",
    "            if not text_content:\n",
    "                continue\n",
    "            \n",
    "            # Prepare metadata\n",
    "            metadata = doc.get('metadata', {}).copy()\n",
    "            metadata['category'] = category\n",
    "            \n",
    "            # Create chunks for this document\n",
    "            chunks = chunker.chunk_text_for_search(text_content, metadata)\n",
    "            \n",
    "            # Convert chunks to search documents\n",
    "            for chunk in chunks:\n",
    "                search_doc = {\n",
    "                    'id': str(uuid.uuid4()),\n",
    "                    'title': f\"{metadata.get('file_name', 'Unknown')} - Part {chunk['chunk_id'] + 1}\",\n",
    "                    'content': chunk['content'],\n",
    "                    'category': category,\n",
    "                    'file_name': metadata.get('file_name', 'Unknown'),\n",
    "                    'file_type': metadata.get('file_type', metadata.get('source_type', 'unknown')),\n",
    "                    'chunk_id': chunk['chunk_id'],\n",
    "                    'chunk_count': chunk['chunk_count'],\n",
    "                    'original_length': metadata.get('original_length', len(text_content)),\n",
    "                    'chunk_length': len(chunk['content']),\n",
    "                    'processing_date': datetime.now().isoformat() + 'Z'\n",
    "                }\n",
    "                search_documents.append(search_doc)\n",
    "    \n",
    "    print(f\"\\n✅ Prepared {len(search_documents)} documents for search index\")\n",
    "    \n",
    "    # Show statistics\n",
    "    category_stats = {}\n",
    "    for doc in search_documents:\n",
    "        category = doc['category']\n",
    "        if category not in category_stats:\n",
    "            category_stats[category] = {'count': 0, 'files': set()}\n",
    "        category_stats[category]['count'] += 1\n",
    "        category_stats[category]['files'].add(doc['file_name'])\n",
    "    \n",
    "    for category, stats in category_stats.items():\n",
    "        print(f\"   - {category}: {stats['count']} chunks from {len(stats['files'])} files\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ No processed documents found. Please run the document processing notebook first.\")\n",
    "    search_documents = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Upload Documents to Azure AI Search with Integrated Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Starting document upload to Azure AI Search...\n",
      "============================================================\n",
      "ℹ️ Azure AI Search will automatically generate embeddings using integrated vectorization\n",
      "📤 Uploading 77 documents to search index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading batches: 100%|██████████| 2/2 [00:00<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Document upload completed!\n",
      "\n",
      "⏳ Waiting for indexing to complete...\n",
      "✅ Index now contains 77 documents\n",
      "📊 Index statistics:\n",
      "   - Documents: 0\n",
      "   - Storage size: 0 bytes\n",
      "   - Vector index size: 0 bytes\n"
     ]
    }
   ],
   "source": [
    "class SearchIndexUploader:\n",
    "    \"\"\"Class to upload documents to Azure AI Search\"\"\"\n",
    "    \n",
    "    def __init__(self, search_client: SearchClient):\n",
    "        self.search_client = search_client\n",
    "    \n",
    "    def upload_documents_batch(self, documents: List[Dict], batch_size: int = 50) -> bool:\n",
    "        \"\"\"Upload documents to search index in batches\"\"\"\n",
    "        try:\n",
    "            total_docs = len(documents)\n",
    "            print(f\"📤 Uploading {total_docs} documents to search index...\")\n",
    "            \n",
    "            # Upload in batches\n",
    "            for i in tqdm(range(0, total_docs, batch_size), desc=\"Uploading batches\"):\n",
    "                batch = documents[i:i + batch_size]\n",
    "                \n",
    "                # Prepare batch for upload (Azure AI Search will handle vectorization)\n",
    "                upload_batch = []\n",
    "                for doc in batch:\n",
    "                    # Remove any fields that shouldn't be in the search document\n",
    "                    search_doc = doc.copy()\n",
    "                    upload_batch.append(search_doc)\n",
    "                \n",
    "                # Upload batch\n",
    "                result = self.search_client.upload_documents(documents=upload_batch)\n",
    "                \n",
    "                # Check for errors\n",
    "                failed_docs = [r for r in result if not r.succeeded]\n",
    "                if failed_docs:\n",
    "                    print(f\"⚠️ Failed to upload {len(failed_docs)} documents in batch {i//batch_size + 1}\")\n",
    "                    for failed in failed_docs[:3]:  # Show first 3 errors\n",
    "                        print(f\"   Error: {failed.error_message}\")\n",
    "            \n",
    "            print(f\"✅ Document upload completed!\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error uploading documents: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def get_document_count(self) -> int:\n",
    "        \"\"\"Get the current document count in the index\"\"\"\n",
    "        try:\n",
    "            # Simple search to get document count\n",
    "            results = self.search_client.search(\"*\", include_total_count=True, top=1)\n",
    "            return results.get_count()\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error getting document count: {e}\")\n",
    "            return 0\n",
    "\n",
    "# Upload documents to search index\n",
    "if search_client and search_documents:\n",
    "    uploader = SearchIndexUploader(search_client)\n",
    "    \n",
    "    print(\"\\n🚀 Starting document upload to Azure AI Search...\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ℹ️ Azure AI Search will automatically generate embeddings using integrated vectorization\")\n",
    "    \n",
    "    success = uploader.upload_documents_batch(search_documents)\n",
    "    \n",
    "    if success:\n",
    "        # Wait a moment for indexing to complete\n",
    "        import time\n",
    "        print(\"\\n⏳ Waiting for indexing to complete...\")\n",
    "        time.sleep(10)\n",
    "        \n",
    "        # Get final document count\n",
    "        doc_count = uploader.get_document_count()\n",
    "        print(f\"✅ Index now contains {doc_count} documents\")\n",
    "        \n",
    "        # Get index statistics\n",
    "        if index_manager:\n",
    "            stats = index_manager.get_index_stats()\n",
    "            if stats:\n",
    "                print(f\"📊 Index statistics:\")\n",
    "                print(f\"   - Documents: {stats.get('document_count', 'N/A')}\")\n",
    "                print(f\"   - Storage size: {stats.get('storage_size', 'N/A')} bytes\")\n",
    "                print(f\"   - Vector index size: {stats.get('vector_index_size', 'N/A')} bytes\")\n",
    "    else:\n",
    "        print(\"❌ Failed to upload documents to search index\")\n",
    "else:\n",
    "    print(\"❌ Cannot upload documents - missing search client or documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test the Search Index with Semantic and Vector Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Search tester initialized\n"
     ]
    }
   ],
   "source": [
    "class SearchTester:\n",
    "    \"\"\"Class to test the search index with various query types\"\"\"\n",
    "    \n",
    "    def __init__(self, search_client: SearchClient):\n",
    "        self.search_client = search_client\n",
    "    \n",
    "    def vector_search(self, query: str, top_k: int = 5, category_filter: str = None) -> List[Dict]:\n",
    "        \"\"\"Perform vector search using integrated vectorization\"\"\"\n",
    "        try:\n",
    "            # Build search parameters\n",
    "            search_params = {\n",
    "                \"search_text\": query,\n",
    "                \"top\": top_k,\n",
    "                \"search_mode\": \"any\",\n",
    "                \"query_type\": \"semantic\",\n",
    "                \"semantic_configuration_name\": \"insurance-semantic\",\n",
    "                \"select\": [\"id\", \"title\", \"content\", \"category\", \"file_name\", \"chunk_id\", \"chunk_count\"]\n",
    "            }\n",
    "            \n",
    "            # Add category filter if specified\n",
    "            if category_filter:\n",
    "                search_params[\"filter\"] = f\"category eq '{category_filter}'\"\n",
    "            \n",
    "            # Perform search\n",
    "            results = self.search_client.search(**search_params)\n",
    "            \n",
    "            # Convert results to list\n",
    "            search_results = []\n",
    "            for result in results:\n",
    "                search_results.append({\n",
    "                    'id': result['id'],\n",
    "                    'title': result['title'],\n",
    "                    'content': result['content'],\n",
    "                    'category': result['category'],\n",
    "                    'file_name': result['file_name'],\n",
    "                    'chunk_id': result['chunk_id'],\n",
    "                    'chunk_count': result['chunk_count'],\n",
    "                    'score': result.get('@search.score', 0),\n",
    "                    'reranker_score': result.get('@search.reranker_score', 0)\n",
    "                })\n",
    "            \n",
    "            return search_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error in vector search: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def hybrid_search(self, query: str, top_k: int = 5) -> List[Dict]:\n",
    "        \"\"\"Perform hybrid search (keyword + vector)\"\"\"\n",
    "        try:\n",
    "            results = self.search_client.search(\n",
    "                search_text=query,\n",
    "                top=top_k,\n",
    "                search_mode=\"all\",\n",
    "                include_total_count=True,\n",
    "                select=[\"id\", \"title\", \"content\", \"category\", \"file_name\", \"chunk_id\"]\n",
    "            )\n",
    "            \n",
    "            search_results = []\n",
    "            for result in results:\n",
    "                search_results.append({\n",
    "                    'id': result['id'],\n",
    "                    'title': result['title'],\n",
    "                    'content': result['content'],\n",
    "                    'category': result['category'],\n",
    "                    'file_name': result['file_name'],\n",
    "                    'chunk_id': result['chunk_id'],\n",
    "                    'score': result.get('@search.score', 0)\n",
    "                })\n",
    "            \n",
    "            return search_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error in hybrid search: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def display_search_results(self, query: str, results: List[Dict], search_type: str = \"Search\"):\n",
    "        \"\"\"Display search results in a formatted way\"\"\"\n",
    "        print(f\"\\n🔍 {search_type} Results for: '{query}'\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        if not results:\n",
    "            print(\"No results found.\")\n",
    "            return\n",
    "        \n",
    "        for i, result in enumerate(results, 1):\n",
    "            score = result.get('score', 0)\n",
    "            reranker_score = result.get('reranker_score', 0)\n",
    "            \n",
    "            print(f\"\\n{i}. 📄 {result['title']}\")\n",
    "            print(f\"   📂 Category: {result['category']}\")\n",
    "            print(f\"   📊 Score: {score:.4f}\", end=\"\")\n",
    "            if reranker_score > 0:\n",
    "                print(f\" | Reranker: {reranker_score:.4f}\")\n",
    "            else:\n",
    "                print()\n",
    "            print(f\"   📝 Chunk {result['chunk_id'] + 1}\")\n",
    "            \n",
    "            # Show preview of content\n",
    "            preview = result['content'][:300]\n",
    "            if len(result['content']) > 300:\n",
    "                preview += \"...\"\n",
    "            print(f\"   💬 Preview: {preview}\")\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "# Initialize search tester\n",
    "if search_client:\n",
    "    search_tester = SearchTester(search_client)\n",
    "    print(\"✅ Search tester initialized\")\n",
    "else:\n",
    "    print(\"❌ Cannot initialize search tester - missing search client\")\n",
    "    search_tester = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test with Sample Insurance Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing Azure AI Search with integrated vectorization...\n",
      "================================================================================\n",
      "\n",
      "\n",
      "🔍 Testing query: 'What is covered under collision insurance?'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found 3 relevant chunks\n",
      "\n",
      "🔍 Semantic Search Results for: 'What is covered under collision insurance?'\n",
      "================================================================================\n",
      "\n",
      "1. 📄 comprehensive_auto_policy.md - Part 1\n",
      "   📂 Category: policies\n",
      "   📊 Score: 3.5886 | Reranker: 2.7784\n",
      "   📝 Chunk 1\n",
      "   💬 Preview: # Comprehensive Auto Insurance Policy **Policy Type:** Comprehensive Auto Insurance **Coverage Category:** Full Coverage **Policy Code:** COMP-AUTO-001 ## Section 1: Coverage Overview This comprehensive auto insurance policy provides extensive protection for your vehicle and liability coverage for d...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "2. 📄 motorcycle_policy.md - Part 2\n",
      "   📂 Category: policies\n",
      "   📊 Score: 1.8000 | Reranker: 2.6203\n",
      "   📝 Chunk 2\n",
      "   💬 Preview: 50cc-250cc) - Medium displacement (251cc-600cc) - Large displacement (601cc-1000cc) - High-performance (1000cc+) - Electric motorcycles (by power rating) ## Section 3: Coverage Components ### Section 3.1: Liability Coverage - Bodily injury to other parties - Property damage to other vehicles and pro...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "3. 📄 comprehensive_auto_policy.md - Part 2\n",
      "   📂 Category: policies\n",
      "   📊 Score: 1.0006 | Reranker: 2.6012\n",
      "   📝 Chunk 2\n",
      "   💬 Preview: 3: Liability Coverage - Bodily injury to third parties - Property damage to other vehicles or structures - Legal defense costs - Court-ordered judgments and settlements ### Section 2.4: Medical Payments Coverage - Medical expenses for driver and passengers - Hospital and emergency room costs - Rehab...\n",
      "--------------------------------------------------------------------------------\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "🔍 Testing query: 'How much does comprehensive coverage cost?'\n",
      "✅ Found 3 relevant chunks\n",
      "\n",
      "🔍 Semantic Search Results for: 'How much does comprehensive coverage cost?'\n",
      "================================================================================\n",
      "\n",
      "1. 📄 comprehensive_auto_policy.md - Part 3\n",
      "   📂 Category: policies\n",
      "   📊 Score: 2.4959 | Reranker: 2.7387\n",
      "   📝 Chunk 3\n",
      "   💬 Preview: 2: Premium Limits (Available) - Collision: Up to $100,000 per incident - Comprehensive: Up to $100,000 per incident - Bodily Injury Liability: Up to $500,000 per person, $1,000,000 per accident - Property Damage Liability: Up to $500,000 per accident - Medical Payments: Up to $50,000 per person ## S...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "2. 📄 high_value_vehicle_policy.md - Part 5\n",
      "   📂 Category: policies\n",
      "   📊 Score: 2.3580 | Reranker: 2.7224\n",
      "   📝 Chunk 5\n",
      "   💬 Preview: ctible Options - Comprehensive: $0, $1,000, $2,500, $5,000 - Collision: $0, $1,000, $2,500, $5,000, $10,000 - Disappearing deductible programs available - Separate deductibles for different coverage types ## Specialized Endorsements ### Track Day and Racing Coverage - Organized track events and driv...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "3. 📄 comprehensive_auto_policy.md - Part 2\n",
      "   📂 Category: policies\n",
      "   📊 Score: 1.5325 | Reranker: 2.7167\n",
      "   📝 Chunk 2\n",
      "   💬 Preview: 3: Liability Coverage - Bodily injury to third parties - Property damage to other vehicles or structures - Legal defense costs - Court-ordered judgments and settlements ### Section 2.4: Medical Payments Coverage - Medical expenses for driver and passengers - Hospital and emergency room costs - Rehab...\n",
      "--------------------------------------------------------------------------------\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "🔍 Testing query: 'What are the liability limits for commercial vehicles?'\n",
      "✅ Found 3 relevant chunks\n",
      "\n",
      "🔍 Semantic Search Results for: 'What are the liability limits for commercial vehicles?'\n",
      "================================================================================\n",
      "\n",
      "1. 📄 commercial_auto_policy.md - Part 4\n",
      "   📂 Category: policies\n",
      "   📊 Score: 5.5956 | Reranker: 2.9351\n",
      "   📝 Chunk 4\n",
      "   💬 Preview: protection while in care, custody, and control - Garage keepers legal liability - False pretense coverage ## Coverage Limits ### Standard Commercial Limits - Bodily Injury Liability: $500,000 per person, $1,000,000 per accident - Property Damage Liability: $500,000 per accident - Medical Payments: $...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "2. 📄 commercial_auto_policy.md - Part 2\n",
      "   📂 Category: policies\n",
      "   📊 Score: 4.2976 | Reranker: 2.6061\n",
      "   📝 Chunk 2\n",
      "   💬 Preview: - Service and repair operations - Sales and delivery services - Construction and contracting - Transportation and logistics - Professional services (real estate, consulting) - Retail and wholesale operations ## Coverage Components ### Liability Coverage - Bodily injury to third parties during busine...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "3. 📄 high_value_vehicle_policy.md - Part 4\n",
      "   📂 Category: policies\n",
      "   📊 Score: 4.0052 | Reranker: 2.3725\n",
      "   📝 Chunk 4\n",
      "   💬 Preview: Appraisal and Valuation Services - Annual professional appraisals - Market value tracking and updates - Documentation and photography services - Condition reports and maintenance records - Authenticity verification for classics ### Risk Management Services - Security system recommendations - Storage...\n",
      "--------------------------------------------------------------------------------\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "🔍 Testing query: 'Does my policy cover theft and vandalism?'\n",
      "✅ Found 3 relevant chunks\n",
      "\n",
      "🔍 Semantic Search Results for: 'Does my policy cover theft and vandalism?'\n",
      "================================================================================\n",
      "\n",
      "1. 📄 comprehensive_auto_policy.md - Part 1\n",
      "   📂 Category: policies\n",
      "   📊 Score: 5.9500 | Reranker: 2.8656\n",
      "   📝 Chunk 1\n",
      "   💬 Preview: # Comprehensive Auto Insurance Policy **Policy Type:** Comprehensive Auto Insurance **Coverage Category:** Full Coverage **Policy Code:** COMP-AUTO-001 ## Section 1: Coverage Overview This comprehensive auto insurance policy provides extensive protection for your vehicle and liability coverage for d...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "2. 📄 motorcycle_policy.md - Part 2\n",
      "   📂 Category: policies\n",
      "   📊 Score: 2.4869 | Reranker: 2.7621\n",
      "   📝 Chunk 2\n",
      "   💬 Preview: 50cc-250cc) - Medium displacement (251cc-600cc) - Large displacement (601cc-1000cc) - High-performance (1000cc+) - Electric motorcycles (by power rating) ## Section 3: Coverage Components ### Section 3.1: Liability Coverage - Bodily injury to other parties - Property damage to other vehicles and pro...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "3. 📄 high_value_vehicle_policy.md - Part 3\n",
      "   📂 Category: policies\n",
      "   📊 Score: 1.0898 | Reranker: 2.7415\n",
      "   📝 Chunk 3\n",
      "   💬 Preview: ing and finish quality guarantees ### Enhanced Comprehensive Coverage - Theft protection with GPS tracking requirements - Vandalism and malicious mischief coverage - Weather damage protection (hail, flood, wind) - Fire and explosion coverage - Falling object protection - Animal collision coverage wi...\n",
      "--------------------------------------------------------------------------------\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "🔍 Testing query: 'What happens if I hit an uninsured driver?'\n",
      "✅ Found 3 relevant chunks\n",
      "\n",
      "🔍 Semantic Search Results for: 'What happens if I hit an uninsured driver?'\n",
      "================================================================================\n",
      "\n",
      "1. 📄 comprehensive_auto_policy.md - Part 2\n",
      "   📂 Category: policies\n",
      "   📊 Score: 3.9700 | Reranker: 2.8287\n",
      "   📝 Chunk 2\n",
      "   💬 Preview: 3: Liability Coverage - Bodily injury to third parties - Property damage to other vehicles or structures - Legal defense costs - Court-ordered judgments and settlements ### Section 2.4: Medical Payments Coverage - Medical expenses for driver and passengers - Hospital and emergency room costs - Rehab...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "2. 📄 motorcycle_policy.md - Part 3\n",
      "   📂 Category: policies\n",
      "   📊 Score: 3.6574 | Reranker: 2.5877\n",
      "   📝 Chunk 3\n",
      "   💬 Preview: 3: Medical Payments Coverage - Medical expenses for rider and passenger - Emergency room and hospital costs - Ambulance transportation - Rehabilitation and physical therapy - Dental and vision care related to accidents ### Section 3.4: Uninsured/Underinsured Motorist Coverage - Protection against un...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "3. 📄 commercial_auto_policy.md - Part 3\n",
      "   📂 Category: policies\n",
      "   📊 Score: 3.4781 | Reranker: 2.3796\n",
      "   📝 Chunk 3\n",
      "   💬 Preview: t ### Medical Payments Coverage - Medical expenses for employees injured in business vehicles - Coverage for business guests and passengers - Emergency medical treatment costs - Ambulance and hospital expenses ### Uninsured Motorist Coverage - Protection against uninsured drivers during business ope...\n",
      "--------------------------------------------------------------------------------\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "🔍 Testing query: 'High value vehicle insurance requirements'\n",
      "✅ Found 3 relevant chunks\n",
      "\n",
      "🔍 Semantic Search Results for: 'High value vehicle insurance requirements'\n",
      "================================================================================\n",
      "\n",
      "1. 📄 high_value_vehicle_policy.md - Part 4\n",
      "   📂 Category: policies\n",
      "   📊 Score: 4.2656 | Reranker: 2.9252\n",
      "   📝 Chunk 4\n",
      "   💬 Preview: Appraisal and Valuation Services - Annual professional appraisals - Market value tracking and updates - Documentation and photography services - Condition reports and maintenance records - Authenticity verification for classics ### Risk Management Services - Security system recommendations - Storage...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "2. 📄 high_value_vehicle_policy.md - Part 1\n",
      "   📂 Category: policies\n",
      "   📊 Score: 5.7113 | Reranker: 2.8564\n",
      "   📝 Chunk 1\n",
      "   💬 Preview: # High-Value Vehicle Insurance Policy **Policy Type:** High-Value Vehicle Insurance **Coverage Category:** Luxury and Exotic Vehicle Coverage **Policy Code:** HV-AUTO-001 ## Coverage Overview This specialized high-value vehicle insurance policy provides comprehensive protection for luxury, exotic, c...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "3. 📄 high_value_vehicle_policy.md - Part 6\n",
      "   📂 Category: policies\n",
      "   📊 Score: 2.9043 | Reranker: 2.8415\n",
      "   📝 Chunk 6\n",
      "   💬 Preview: arts - Tools and equipment coverage - Memorabilia and documentation ## Underwriting Requirements ### Vehicle Documentation - Professional appraisal (required for vehicles over $100,000) - Detailed photographs and condition reports - Maintenance records and service history - Modification documentatio...\n",
      "--------------------------------------------------------------------------------\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "🔍 Testing query: 'Motorcycle insurance coverage options'\n",
      "✅ Found 3 relevant chunks\n",
      "\n",
      "🔍 Semantic Search Results for: 'Motorcycle insurance coverage options'\n",
      "================================================================================\n",
      "\n",
      "1. 📄 motorcycle_policy.md - Part 3\n",
      "   📂 Category: policies\n",
      "   📊 Score: 3.0990 | Reranker: 3.2151\n",
      "   📝 Chunk 3\n",
      "   💬 Preview: 3: Medical Payments Coverage - Medical expenses for rider and passenger - Emergency room and hospital costs - Ambulance transportation - Rehabilitation and physical therapy - Dental and vision care related to accidents ### Section 3.4: Uninsured/Underinsured Motorist Coverage - Protection against un...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "2. 📄 motorcycle_policy.md - Part 2\n",
      "   📂 Category: policies\n",
      "   📊 Score: 3.2383 | Reranker: 3.1506\n",
      "   📝 Chunk 2\n",
      "   💬 Preview: 50cc-250cc) - Medium displacement (251cc-600cc) - Large displacement (601cc-1000cc) - High-performance (1000cc+) - Electric motorcycles (by power rating) ## Section 3: Coverage Components ### Section 3.1: Liability Coverage - Bodily injury to other parties - Property damage to other vehicles and pro...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "3. 📄 motorcycle_policy.md - Part 6\n",
      "   📂 Category: policies\n",
      "   📊 Score: 4.8702 | Reranker: 3.0648\n",
      "   📝 Chunk 6\n",
      "   💬 Preview: rage - Extended trip protection - Out-of-state coverage enhancement - International travel (Canada/Mexico) - Camping and touring equipment - Emergency transportation home ### Antique and Classic Motorcycle Coverage - Agreed value coverage for classics - Show and exhibition coverage - Restoration cov...\n",
      "--------------------------------------------------------------------------------\n",
      "----------------------------------------\n",
      "\n",
      "✅ Query testing completed!\n"
     ]
    }
   ],
   "source": [
    "# Test the search index with sample queries\n",
    "if search_tester:\n",
    "    test_queries = [\n",
    "        \"What is covered under collision insurance?\",\n",
    "        \"How much does comprehensive coverage cost?\", \n",
    "        \"What are the liability limits for commercial vehicles?\",\n",
    "        \"Does my policy cover theft and vandalism?\",\n",
    "        \"What happens if I hit an uninsured driver?\",\n",
    "        \"High value vehicle insurance requirements\",\n",
    "        \"Motorcycle insurance coverage options\"\n",
    "    ]\n",
    "    \n",
    "    print(\"🧪 Testing Azure AI Search with integrated vectorization...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for query in test_queries:\n",
    "        print(f\"\\n\\n🔍 Testing query: '{query}'\")\n",
    "        \n",
    "        # Test semantic search\n",
    "        results = search_tester.vector_search(query, top_k=3)\n",
    "        \n",
    "        if results:\n",
    "            print(f\"✅ Found {len(results)} relevant chunks\")\n",
    "            search_tester.display_search_results(query, results, \"Semantic Search\")\n",
    "        else:\n",
    "            print(\"❌ No relevant documents found\")\n",
    "        \n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    print(\"\\n✅ Query testing completed!\")\n",
    "else:\n",
    "    print(\"❌ Cannot test queries - search tester not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Interactive Search Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_search():\n",
    "    \"\"\"Interactive search interface for testing\"\"\"\n",
    "    if not search_tester:\n",
    "        print(\"❌ Search tester not available\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n🔍 Interactive Azure AI Search Interface\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Enter your search queries (type 'quit' to exit)\")\n",
    "    print(\"Optional commands:\")\n",
    "    print(\"  - Add 'category:policies' or 'category:claims' to filter results\")\n",
    "    print(\"  - Use natural language queries for best semantic search results\")\n",
    "    print()\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            query = input(\"\\n🔍 Search: \").strip()\n",
    "            \n",
    "            if query.lower() in ['quit', 'exit', 'q']:\n",
    "                break\n",
    "            \n",
    "            if not query:\n",
    "                continue\n",
    "            \n",
    "            # Check for category filter\n",
    "            category_filter = None\n",
    "            if 'category:' in query:\n",
    "                parts = query.split('category:')\n",
    "                query = parts[0].strip()\n",
    "                category_filter = parts[1].strip()\n",
    "            \n",
    "            # Perform semantic search\n",
    "            print(f\"\\n🧠 Performing semantic search...\")\n",
    "            results = search_tester.vector_search(query, top_k=5, category_filter=category_filter)\n",
    "            \n",
    "            # Display results\n",
    "            search_tester.display_search_results(query, results, \"Semantic Search\")\n",
    "            \n",
    "            # Also try hybrid search for comparison\n",
    "            print(f\"\\n🔄 Hybrid search results:\")\n",
    "            hybrid_results = search_tester.hybrid_search(query, top_k=3)\n",
    "            if hybrid_results:\n",
    "                for i, result in enumerate(hybrid_results[:2], 1):  # Show top 2\n",
    "                    print(f\"{i}. {result['title']} (Score: {result['score']:.4f})\")\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error: {e}\")\n",
    "    \n",
    "    print(\"\\n👋 Search session ended\")\n",
    "\n",
    "# Note: Uncomment the line below to start interactive search\n",
    "# interactive_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 AZURE AI SEARCH INTEGRATION SUMMARY\n",
      "============================================================\n",
      "📅 Completed: 2025-07-23T12:05:51.970883\n",
      "🔍 Search Service: msagthack-search-kl2yy7velhg4u\n",
      "📇 Search Index: insurance-documents-index\n",
      "📄 Documents Processed: 11\n",
      "🗂️ Chunks Indexed: 77\n",
      "🤖 Embedding Model: text-embedding-ada-002\n",
      "⚡ Vectorization: Azure AI Search Integrated Vectorization\n",
      "\n",
      "🚀 SEARCH CAPABILITIES:\n",
      "  ✅ Semantic Search\n",
      "  ✅ Vector Search\n",
      "  ✅ Hybrid Search\n",
      "  ✅ Automatic Vectorization\n",
      "  ✅ Real Time Indexing\n",
      "\n",
      "📊 BY CATEGORY:\n",
      "  • Policies:\n",
      "    - Files: 5\n",
      "    - Chunks: 44\n",
      "    - Total Characters: 37,186\n",
      "  • Claims:\n",
      "    - Files: 6\n",
      "    - Chunks: 18\n",
      "    - Total Characters: 15,675\n",
      "  • Statements:\n",
      "    - Files: 5\n",
      "    - Chunks: 15\n",
      "    - Total Characters: 10,140\n"
     ]
    }
   ],
   "source": [
    "# Generate final summary\n",
    "def generate_summary():\n",
    "    \"\"\"Generate a comprehensive summary of the Azure AI Search integration\"\"\"\n",
    "    \n",
    "    # Get index statistics\n",
    "    index_stats = {}\n",
    "    doc_count = 0\n",
    "    \n",
    "    if search_client and index_manager:\n",
    "        try:\n",
    "            doc_count = SearchIndexUploader(search_client).get_document_count()\n",
    "            index_stats = index_manager.get_index_stats()\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    summary = {\n",
    "        \"integration_summary\": {\n",
    "            \"completion_date\": datetime.now().isoformat(),\n",
    "            \"search_service\": Config.SEARCH_SERVICE_NAME,\n",
    "            \"search_index\": Config.SEARCH_INDEX_NAME,\n",
    "            \"total_documents_processed\": len(processed_documents.get('policies', []) + processed_documents.get('claims', [])) if 'processed_documents' in globals() and processed_documents else 0,\n",
    "            \"total_chunks_indexed\": doc_count,\n",
    "            \"embedding_model\": Config.AZURE_OPENAI_EMBEDDING_DEPLOYMENT,\n",
    "            \"vectorization_method\": \"Azure AI Search Integrated Vectorization\",\n",
    "            \"chunk_configuration\": {\n",
    "                \"chunk_size\": Config.CHUNK_SIZE,\n",
    "                \"chunk_overlap\": Config.CHUNK_OVERLAP\n",
    "            }\n",
    "        },\n",
    "        \"search_capabilities\": {\n",
    "            \"semantic_search\": True,\n",
    "            \"vector_search\": True,\n",
    "            \"hybrid_search\": True,\n",
    "            \"automatic_vectorization\": True,\n",
    "            \"real_time_indexing\": True\n",
    "        },\n",
    "        \"index_statistics\": index_stats,\n",
    "        \"ready_for_ai_agents\": bool(doc_count > 0)\n",
    "    }\n",
    "    \n",
    "    # Add category breakdown if available\n",
    "    if 'search_documents' in globals() and search_documents:\n",
    "        category_stats = {}\n",
    "        for doc in search_documents:\n",
    "            category = doc['category']\n",
    "            if category not in category_stats:\n",
    "                category_stats[category] = {\n",
    "                    'chunks': 0,\n",
    "                    'total_characters': 0,\n",
    "                    'files': set()\n",
    "                }\n",
    "            \n",
    "            category_stats[category]['chunks'] += 1\n",
    "            category_stats[category]['total_characters'] += doc['chunk_length']\n",
    "            category_stats[category]['files'].add(doc['file_name'])\n",
    "        \n",
    "        # Convert sets to lists and add unique file counts\n",
    "        for category in category_stats:\n",
    "            category_stats[category]['files'] = list(category_stats[category]['files'])\n",
    "            category_stats[category]['unique_files'] = len(category_stats[category]['files'])\n",
    "        \n",
    "        summary['categories_processed'] = category_stats\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Generate and display summary\n",
    "final_summary = generate_summary()\n",
    "\n",
    "print(\"📋 AZURE AI SEARCH INTEGRATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"📅 Completed: {final_summary['integration_summary']['completion_date']}\")\n",
    "print(f\"🔍 Search Service: {final_summary['integration_summary']['search_service']}\")\n",
    "print(f\"📇 Search Index: {final_summary['integration_summary']['search_index']}\")\n",
    "print(f\"📄 Documents Processed: {final_summary['integration_summary']['total_documents_processed']}\")\n",
    "print(f\"🗂️ Chunks Indexed: {final_summary['integration_summary']['total_chunks_indexed']}\")\n",
    "print(f\"🤖 Embedding Model: {final_summary['integration_summary']['embedding_model']}\")\n",
    "print(f\"⚡ Vectorization: {final_summary['integration_summary']['vectorization_method']}\")\n",
    "\n",
    "print(\"\\n🚀 SEARCH CAPABILITIES:\")\n",
    "capabilities = final_summary['search_capabilities']\n",
    "for capability, enabled in capabilities.items():\n",
    "    status = \"✅\" if enabled else \"❌\"\n",
    "    print(f\"  {status} {capability.replace('_', ' ').title()}\")\n",
    "\n",
    "if 'categories_processed' in final_summary:\n",
    "    print(\"\\n📊 BY CATEGORY:\")\n",
    "    for category, stats in final_summary['categories_processed'].items():\n",
    "        print(f\"  • {category.title()}:\")\n",
    "        print(f\"    - Files: {stats['unique_files']}\")\n",
    "        print(f\"    - Chunks: {stats['chunks']}\")\n",
    "        print(f\"    - Total Characters: {stats['total_characters']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! 🎉\n",
    "\n",
    "You have successfully integrated your documents with Azure AI Search using integrated vectorization!\n",
    "\n",
    "### What you've accomplished:\n",
    "1. ✅ **Created Azure AI Search Index** with integrated vectorization\n",
    "2. ✅ **Uploaded documents** directly to Azure AI Search\n",
    "3. ✅ **Enabled automatic vectorization** using Azure OpenAI embeddings\n",
    "4. ✅ **Implemented semantic search** capabilities\n",
    "5. ✅ **Tested search functionality** with insurance queries\n",
    "\n",
    "### Key advantages of this approach:\n",
    "- 🚀 **No local vector storage** - everything handled by Azure AI Search\n",
    "- ⚡ **Automatic vectorization** - Azure AI Search generates embeddings automatically\n",
    "- 🔍 **Advanced search capabilities** - semantic, vector, and hybrid search\n",
    "- 📈 **Scalable and managed** - Azure handles all the infrastructure\n",
    "- 🔒 **Secure and compliant** - Enterprise-grade security and compliance\n",
    "\n",
    "### Your search index is now ready for:\n",
    "- 🤖 **AI Agent Integration** - Direct connection to Azure AI Agent Service\n",
    "- 🔍 **Real-time Search** - Instant semantic and vector search\n",
    "- 📊 **Analytics and Insights** - Built-in search analytics\n",
    "- 🔄 **Dynamic Updates** - Real-time document indexing\n",
    "\n",
    "**Next:** Proceed to Challenge 2 to build AI agents that will use this search index!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
